[06-16 23:20:34] INFO - ==== Train Arguments ==== {
  "output_dir": "../ckpts/Roberta_GP_2022",
  "overwrite_output_dir": true,
  "do_train": true,
  "do_eval": true,
  "do_predict": false,
  "evaluation_strategy": "steps",
  "prediction_loss_only": false,
  "per_device_train_batch_size": 4,
  "per_device_eval_batch_size": 16,
  "per_gpu_train_batch_size": null,
  "per_gpu_eval_batch_size": null,
  "gradient_accumulation_steps": 4,
  "eval_accumulation_steps": 500,
  "eval_delay": 0,
  "learning_rate": 2e-05,
  "weight_decay": 3e-06,
  "adam_beta1": 0.9,
  "adam_beta2": 0.999,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 0.5,
  "num_train_epochs": 30.0,
  "max_steps": -1,
  "lr_scheduler_type": "cosine",
  "warmup_ratio": 0.05,
  "warmup_steps": 0,
  "log_level": -1,
  "log_level_replica": -1,
  "log_on_each_node": true,
  "logging_dir": "../ckpts/Roberta_GP_2022",
  "logging_strategy": "steps",
  "logging_first_step": true,
  "logging_steps": 200,
  "logging_nan_inf_filter": true,
  "save_strategy": "steps",
  "save_steps": 1000,
  "save_total_limit": 1,
  "save_on_each_node": false,
  "no_cuda": false,
  "seed": 2022,
  "data_seed": null,
  "bf16": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "half_precision_backend": "auto",
  "bf16_full_eval": false,
  "fp16_full_eval": false,
  "tf32": null,
  "local_rank": -1,
  "xpu_backend": null,
  "tpu_num_cores": null,
  "tpu_metrics_debug": false,
  "debug": [],
  "dataloader_drop_last": false,
  "eval_steps": 1000,
  "dataloader_num_workers": 8,
  "past_index": -1,
  "run_name": "../ckpts/Roberta_GP_2022",
  "disable_tqdm": true,
  "remove_unused_columns": true,
  "label_names": [
    "labels_"
  ],
  "load_best_model_at_end": true,
  "metric_for_best_model": "f1",
  "greater_is_better": true,
  "ignore_data_skip": false,
  "sharded_ddp": [],
  "fsdp": [],
  "fsdp_min_num_params": 0,
  "deepspeed": null,
  "label_smoothing_factor": 0.0,
  "optim": "adamw_hf",
  "adafactor": false,
  "group_by_length": false,
  "length_column_name": "length",
  "report_to": [],
  "ddp_find_unused_parameters": null,
  "ddp_bucket_cap_mb": null,
  "dataloader_pin_memory": false,
  "skip_memory_metrics": true,
  "use_legacy_prediction_loop": false,
  "push_to_hub": false,
  "resume_from_checkpoint": null,
  "hub_model_id": null,
  "hub_strategy": "every_save",
  "hub_token": "<HUB_TOKEN>",
  "hub_private_repo": false,
  "gradient_checkpointing": false,
  "include_inputs_for_metrics": false,
  "fp16_backend": "auto",
  "push_to_hub_model_id": null,
  "push_to_hub_organization": null,
  "push_to_hub_token": "<PUSH_TO_HUB_TOKEN>",
  "_n_gpu": 1,
  "mp_parameters": "",
  "auto_find_batch_size": false,
  "full_determinism": false
}
[06-16 23:20:34] INFO - ==== Model Arguments ==== {
  "model_type": "Roberta",
  "head_type": "GP",
  "adv_train": "None",
  "model_path": "/DB/rhome/yuhaowang/RoBERTa_zh_Large_PyTorch",
  "init_model": 0,
  "lr_decay_rate": -1.0
}
[06-16 23:20:34] INFO - ==== Data Arguments ==== {
  "cblue_root": "../data/CBLUEDatasets",
  "max_length": 512,
  "unimodel_path": "../pretrain_model/gigaword_chn.all.a2b.uni.ite50.vec",
  "bimodel_path": "../pretrain_model/gigaword_chn.all.a2b.bi.ite50.vec",
  "wordmodel_path": "../pretrain_model/ctb.50d.vec"
}
[06-16 23:20:34] INFO - ==== FLAT Arguments ==== {
  "hidden_size": 200,
  "ff_size": 800,
  "num_layers": 8,
  "num_heads": 1,
  "shared_pos_encoding": true
}
[06-16 23:20:41] INFO - ==== Train Arguments ==== {
  "output_dir": "../ckpts/Roberta_GP_2022",
  "overwrite_output_dir": true,
  "do_train": true,
  "do_eval": true,
  "do_predict": false,
  "evaluation_strategy": "steps",
  "prediction_loss_only": false,
  "per_device_train_batch_size": 4,
  "per_device_eval_batch_size": 16,
  "per_gpu_train_batch_size": null,
  "per_gpu_eval_batch_size": null,
  "gradient_accumulation_steps": 4,
  "eval_accumulation_steps": 500,
  "eval_delay": 0,
  "learning_rate": 2e-05,
  "weight_decay": 3e-06,
  "adam_beta1": 0.9,
  "adam_beta2": 0.999,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 0.5,
  "num_train_epochs": 1.0,
  "max_steps": -1,
  "lr_scheduler_type": "cosine",
  "warmup_ratio": 0.05,
  "warmup_steps": 0,
  "log_level": -1,
  "log_level_replica": -1,
  "log_on_each_node": true,
  "logging_dir": "../ckpts/Roberta_GP_2022",
  "logging_strategy": "steps",
  "logging_first_step": true,
  "logging_steps": 200,
  "logging_nan_inf_filter": true,
  "save_strategy": "steps",
  "save_steps": 1000,
  "save_total_limit": 1,
  "save_on_each_node": false,
  "no_cuda": false,
  "seed": 2022,
  "data_seed": null,
  "bf16": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "half_precision_backend": "auto",
  "bf16_full_eval": false,
  "fp16_full_eval": false,
  "tf32": null,
  "local_rank": -1,
  "xpu_backend": null,
  "tpu_num_cores": null,
  "tpu_metrics_debug": false,
  "debug": [],
  "dataloader_drop_last": false,
  "eval_steps": 1000,
  "dataloader_num_workers": 8,
  "past_index": -1,
  "run_name": "../ckpts/Roberta_GP_2022",
  "disable_tqdm": true,
  "remove_unused_columns": true,
  "label_names": [
    "labels_"
  ],
  "load_best_model_at_end": true,
  "metric_for_best_model": "f1",
  "greater_is_better": true,
  "ignore_data_skip": false,
  "sharded_ddp": [],
  "fsdp": [],
  "fsdp_min_num_params": 0,
  "deepspeed": null,
  "label_smoothing_factor": 0.0,
  "optim": "adamw_hf",
  "adafactor": false,
  "group_by_length": false,
  "length_column_name": "length",
  "report_to": [],
  "ddp_find_unused_parameters": null,
  "ddp_bucket_cap_mb": null,
  "dataloader_pin_memory": false,
  "skip_memory_metrics": true,
  "use_legacy_prediction_loop": false,
  "push_to_hub": false,
  "resume_from_checkpoint": null,
  "hub_model_id": null,
  "hub_strategy": "every_save",
  "hub_token": "<HUB_TOKEN>",
  "hub_private_repo": false,
  "gradient_checkpointing": false,
  "include_inputs_for_metrics": false,
  "fp16_backend": "auto",
  "push_to_hub_model_id": null,
  "push_to_hub_organization": null,
  "push_to_hub_token": "<PUSH_TO_HUB_TOKEN>",
  "_n_gpu": 1,
  "mp_parameters": "",
  "auto_find_batch_size": false,
  "full_determinism": false
}
[06-16 23:20:41] INFO - ==== Model Arguments ==== {
  "model_type": "Roberta",
  "head_type": "GP",
  "adv_train": "None",
  "model_path": "/DB/rhome/yuhaowang/RoBERTa_zh_Large_PyTorch",
  "init_model": 0,
  "lr_decay_rate": -1.0
}
[06-16 23:20:41] INFO - ==== Data Arguments ==== {
  "cblue_root": "../data/CBLUEDatasets",
  "max_length": 512,
  "unimodel_path": "../pretrain_model/gigaword_chn.all.a2b.uni.ite50.vec",
  "bimodel_path": "../pretrain_model/gigaword_chn.all.a2b.bi.ite50.vec",
  "wordmodel_path": "../pretrain_model/ctb.50d.vec"
}
[06-16 23:20:41] INFO - ==== FLAT Arguments ==== {
  "hidden_size": 200,
  "ff_size": 800,
  "num_layers": 8,
  "num_heads": 1,
  "shared_pos_encoding": true
}
[06-16 23:20:46] INFO - Trainset: 15000 samples
[06-16 23:20:46] INFO - Devset: 5000 samples
[06-16 23:20:54] INFO - ***** Running training *****
[06-16 23:20:54] INFO -   Num examples = 15000
[06-16 23:20:54] INFO -   Num Epochs = 1
[06-16 23:20:54] INFO -   Instantaneous batch size per device = 4
[06-16 23:20:54] INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
[06-16 23:20:54] INFO -   Gradient Accumulation steps = 4
[06-16 23:20:54] INFO -   Total optimization steps = 937
[06-16 23:21:27] INFO - Keyboard interrupt
[06-16 23:22:46] INFO - ==== Train Arguments ==== {
  "output_dir": "../ckpts/Roberta_GP_2022",
  "overwrite_output_dir": true,
  "do_train": true,
  "do_eval": true,
  "do_predict": false,
  "evaluation_strategy": "steps",
  "prediction_loss_only": false,
  "per_device_train_batch_size": 4,
  "per_device_eval_batch_size": 16,
  "per_gpu_train_batch_size": null,
  "per_gpu_eval_batch_size": null,
  "gradient_accumulation_steps": 4,
  "eval_accumulation_steps": 500,
  "eval_delay": 0,
  "learning_rate": 2e-05,
  "weight_decay": 3e-06,
  "adam_beta1": 0.9,
  "adam_beta2": 0.999,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 0.5,
  "num_train_epochs": 1.0,
  "max_steps": -1,
  "lr_scheduler_type": "cosine",
  "warmup_ratio": 0.05,
  "warmup_steps": 0,
  "log_level": -1,
  "log_level_replica": -1,
  "log_on_each_node": true,
  "logging_dir": "../ckpts/Roberta_GP_2022",
  "logging_strategy": "steps",
  "logging_first_step": true,
  "logging_steps": 200,
  "logging_nan_inf_filter": true,
  "save_strategy": "steps",
  "save_steps": 1000,
  "save_total_limit": 1,
  "save_on_each_node": false,
  "no_cuda": false,
  "seed": 2022,
  "data_seed": null,
  "bf16": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "half_precision_backend": "auto",
  "bf16_full_eval": false,
  "fp16_full_eval": false,
  "tf32": null,
  "local_rank": -1,
  "xpu_backend": null,
  "tpu_num_cores": null,
  "tpu_metrics_debug": false,
  "debug": [],
  "dataloader_drop_last": false,
  "eval_steps": 1000,
  "dataloader_num_workers": 8,
  "past_index": -1,
  "run_name": "../ckpts/Roberta_GP_2022",
  "disable_tqdm": true,
  "remove_unused_columns": true,
  "label_names": [
    "labels_"
  ],
  "load_best_model_at_end": true,
  "metric_for_best_model": "f1",
  "greater_is_better": true,
  "ignore_data_skip": false,
  "sharded_ddp": [],
  "fsdp": [],
  "fsdp_min_num_params": 0,
  "deepspeed": null,
  "label_smoothing_factor": 0.0,
  "optim": "adamw_hf",
  "adafactor": false,
  "group_by_length": false,
  "length_column_name": "length",
  "report_to": [],
  "ddp_find_unused_parameters": null,
  "ddp_bucket_cap_mb": null,
  "dataloader_pin_memory": false,
  "skip_memory_metrics": true,
  "use_legacy_prediction_loop": false,
  "push_to_hub": false,
  "resume_from_checkpoint": null,
  "hub_model_id": null,
  "hub_strategy": "every_save",
  "hub_token": "<HUB_TOKEN>",
  "hub_private_repo": false,
  "gradient_checkpointing": false,
  "include_inputs_for_metrics": false,
  "fp16_backend": "auto",
  "push_to_hub_model_id": null,
  "push_to_hub_organization": null,
  "push_to_hub_token": "<PUSH_TO_HUB_TOKEN>",
  "_n_gpu": 1,
  "mp_parameters": "",
  "auto_find_batch_size": false,
  "full_determinism": false
}
[06-16 23:22:46] INFO - ==== Model Arguments ==== {
  "model_type": "Roberta",
  "head_type": "GP",
  "adv_train": "None",
  "model_path": "/DB/rhome/yuhaowang/RoBERTa_zh_Large_PyTorch",
  "init_model": 0,
  "lr_decay_rate": -1.0
}
[06-16 23:22:46] INFO - ==== Data Arguments ==== {
  "cblue_root": "../data/CBLUEDatasets",
  "max_length": 512,
  "unimodel_path": "../pretrain_model/gigaword_chn.all.a2b.uni.ite50.vec",
  "bimodel_path": "../pretrain_model/gigaword_chn.all.a2b.bi.ite50.vec",
  "wordmodel_path": "../pretrain_model/ctb.50d.vec"
}
[06-16 23:22:46] INFO - ==== FLAT Arguments ==== {
  "hidden_size": 200,
  "ff_size": 800,
  "num_layers": 8,
  "num_heads": 1,
  "shared_pos_encoding": true
}
[06-16 23:22:51] INFO - Trainset: 15000 samples
[06-16 23:22:51] INFO - Devset: 5000 samples
[06-16 23:22:58] INFO - ***** Running training *****
[06-16 23:22:58] INFO -   Num examples = 15000
[06-16 23:22:58] INFO -   Num Epochs = 1
[06-16 23:22:58] INFO -   Instantaneous batch size per device = 4
[06-16 23:22:58] INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
[06-16 23:22:58] INFO -   Gradient Accumulation steps = 4
[06-16 23:22:58] INFO -   Total optimization steps = 937
[06-16 23:28:13] INFO - 

Training completed. Do not forget to share your model on huggingface.co/models =)


[06-16 23:29:35] INFO - ==== Train Arguments ==== {
  "output_dir": "../ckpts/Roberta_GP_2022",
  "overwrite_output_dir": true,
  "do_train": true,
  "do_eval": true,
  "do_predict": false,
  "evaluation_strategy": "steps",
  "prediction_loss_only": false,
  "per_device_train_batch_size": 4,
  "per_device_eval_batch_size": 16,
  "per_gpu_train_batch_size": null,
  "per_gpu_eval_batch_size": null,
  "gradient_accumulation_steps": 4,
  "eval_accumulation_steps": 500,
  "eval_delay": 0,
  "learning_rate": 2e-05,
  "weight_decay": 3e-06,
  "adam_beta1": 0.9,
  "adam_beta2": 0.999,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 0.5,
  "num_train_epochs": 30.0,
  "max_steps": -1,
  "lr_scheduler_type": "cosine",
  "warmup_ratio": 0.05,
  "warmup_steps": 0,
  "log_level": -1,
  "log_level_replica": -1,
  "log_on_each_node": true,
  "logging_dir": "../ckpts/Roberta_GP_2022",
  "logging_strategy": "steps",
  "logging_first_step": true,
  "logging_steps": 200,
  "logging_nan_inf_filter": true,
  "save_strategy": "steps",
  "save_steps": 1000,
  "save_total_limit": 1,
  "save_on_each_node": false,
  "no_cuda": false,
  "seed": 2022,
  "data_seed": null,
  "bf16": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "half_precision_backend": "auto",
  "bf16_full_eval": false,
  "fp16_full_eval": false,
  "tf32": null,
  "local_rank": -1,
  "xpu_backend": null,
  "tpu_num_cores": null,
  "tpu_metrics_debug": false,
  "debug": [],
  "dataloader_drop_last": false,
  "eval_steps": 1000,
  "dataloader_num_workers": 8,
  "past_index": -1,
  "run_name": "../ckpts/Roberta_GP_2022",
  "disable_tqdm": true,
  "remove_unused_columns": true,
  "label_names": [
    "labels_"
  ],
  "load_best_model_at_end": true,
  "metric_for_best_model": "f1",
  "greater_is_better": true,
  "ignore_data_skip": false,
  "sharded_ddp": [],
  "fsdp": [],
  "fsdp_min_num_params": 0,
  "deepspeed": null,
  "label_smoothing_factor": 0.0,
  "optim": "adamw_hf",
  "adafactor": false,
  "group_by_length": false,
  "length_column_name": "length",
  "report_to": [],
  "ddp_find_unused_parameters": null,
  "ddp_bucket_cap_mb": null,
  "dataloader_pin_memory": false,
  "skip_memory_metrics": true,
  "use_legacy_prediction_loop": false,
  "push_to_hub": false,
  "resume_from_checkpoint": null,
  "hub_model_id": null,
  "hub_strategy": "every_save",
  "hub_token": "<HUB_TOKEN>",
  "hub_private_repo": false,
  "gradient_checkpointing": false,
  "include_inputs_for_metrics": false,
  "fp16_backend": "auto",
  "push_to_hub_model_id": null,
  "push_to_hub_organization": null,
  "push_to_hub_token": "<PUSH_TO_HUB_TOKEN>",
  "_n_gpu": 1,
  "mp_parameters": "",
  "auto_find_batch_size": false,
  "full_determinism": false
}
[06-16 23:29:35] INFO - ==== Model Arguments ==== {
  "model_type": "Roberta",
  "head_type": "GP",
  "adv_train": "None",
  "model_path": "/DB/rhome/yuhaowang/RoBERTa_zh_Large_PyTorch",
  "init_model": 0,
  "lr_decay_rate": -1.0
}
[06-16 23:29:35] INFO - ==== Data Arguments ==== {
  "cblue_root": "../data/CBLUEDatasets",
  "max_length": 512,
  "unimodel_path": "../pretrain_model/gigaword_chn.all.a2b.uni.ite50.vec",
  "bimodel_path": "../pretrain_model/gigaword_chn.all.a2b.bi.ite50.vec",
  "wordmodel_path": "../pretrain_model/ctb.50d.vec"
}
[06-16 23:29:35] INFO - ==== FLAT Arguments ==== {
  "hidden_size": 200,
  "ff_size": 800,
  "num_layers": 8,
  "num_heads": 1,
  "shared_pos_encoding": true
}
[06-16 23:29:39] INFO - Trainset: 15000 samples
[06-16 23:29:39] INFO - Devset: 5000 samples
[06-16 23:29:47] INFO - ***** Running training *****
[06-16 23:29:47] INFO -   Num examples = 15000
[06-16 23:29:47] INFO -   Num Epochs = 30
[06-16 23:29:47] INFO -   Instantaneous batch size per device = 4
[06-16 23:29:47] INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
[06-16 23:29:47] INFO -   Gradient Accumulation steps = 4
[06-16 23:29:47] INFO -   Total optimization steps = 28110
[06-16 23:35:18] INFO - ***** Running Evaluation *****
[06-16 23:35:18] INFO -   Num examples = 5000
[06-16 23:35:18] INFO -   Batch size = 16
[06-16 23:36:09] INFO - Saving model checkpoint to ../ckpts/Roberta_GP_2022/checkpoint-1000
[06-16 23:36:09] INFO - Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[06-16 23:39:12] INFO - Keyboard interrupt
